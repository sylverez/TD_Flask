# -*- coding: utf-8 -*-
"""NLP_tweet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TPhfmxcBz7TmnMWDXjz163cjkoWWXhGX
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv("tweet_data.csv")


"""*   class 0 -> hate_speech
*   class 1 -> offensive_language
*   class 2 -> neither

# NLP

inspired from https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34
"""

from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import model_selection, naive_bayes, svm
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis



def warn(*args, **kwargs): pass
import warnings
warnings.warn = warn

np.random.seed(42)

Corpus = data

"""## Corpus-preprocessing : Tokenization, Word stemming/Lemmatization"""

# Step - a : Remove blank rows if any.
Corpus['tweet'].dropna(inplace=True)

# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently
Corpus['tweet'] = [entry.lower() for entry in Corpus['tweet']]

import nltk
nltk.download('punkt')

# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words
Corpus['tweet']= [word_tokenize(entry) for entry in Corpus['tweet']]

Corpus['tweet'].head()

nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.
# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun
tag_map = defaultdict(lambda : wn.NOUN)
tag_map['J'] = wn.ADJ
tag_map['V'] = wn.VERB
tag_map['R'] = wn.ADV 

for index,entry in enumerate(Corpus['tweet']):
  
    # Declaring Empty List to store the words that follow the rules for this step
    Final_words = []
    
    # Initializing WordNetLemmatizer()
    word_Lemmatized = WordNetLemmatizer()
    
    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.
    for word, tag in pos_tag(entry):
      
        # Below condition is to check for Stop words and consider only alphabets
        if word not in stopwords.words('english') and word.isalpha():
            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
            Final_words.append(word_Final)
            
    # The final processed set of words for each iteration will be stored in 'text_final'
    Corpus.loc[index,'text_final'] = str(Final_words)

Corpus.head()

"""## Train/Test split"""

# split 70%/30%
Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['class'],test_size=0.3)

"""## Word Vectorization : TF-IDF

*    Term Frequency: This summarizes how often a given word appears within a document.
*    Inverse Document Frequency: This down scales words that appear a lot across documents.

i.e. TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents
"""

Tfidf_vect = TfidfVectorizer(max_features=5000)
Tfidf_vect.fit(Corpus['text_final'])

Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)

"""TF-IDF build a vocabulary of words which it has learned from the corpus data and it will assign a unique integer number to each of these words. Their will be maximum of 5000 unique words/features as we have set parameter max_features=5000.

Finally we will transform Train_X and Test_X to vectorized Train_X_Tfidf and Test_X_Tfidf. These will now contain for each row a list of unique integer number and its associated importance as calculated by TF-IDF.
"""

# Vocabulary learn from the corpus
print(Tfidf_vect.vocabulary_)

# Vectorized data 
print(Train_X_Tfidf)

#First nbr: Row number of ‘Train_X_Tfidf’ 
#Second nbr: Unique Integer number of each word in the first row
#Third nbr: Score calculated by TF-IDF Vectorizer


"""## Classification algorithm : SVM"""

# Support vector classification with polynomial (degree 3) kernel

# Fit the training dataset on the classifier
SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')
SVM.fit(Train_X_Tfidf,Train_Y)

# Predict the labels on validation dataset
predictions_SVM = SVM.predict(Test_X_Tfidf)

# Use accuracy_score function to get the accuracy
print("SVM Accuracy Score -> ",accuracy_score(predictions_SVM, Test_Y)*100)

#PERSISTENCE TIME


import pickle

pickle.dump(SVM, open('model.pkl','wb'))
